{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd25cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "GPUs Count: 1\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU') if str(device) == \"cuda:0\" else print('GPU not Detected - CPU Selected')\n",
    "print(f\"GPUs Count: {torch.cuda.device_count()}\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e409a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Select Image Pair\n",
    "\n",
    "IMAGE_PAIR = \"CONES\"     ### Choices:     CONES     DOLLS     MOEBIUS     ROCKS     CLOTH\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "if IMAGE_PAIR == \"CONES\":\n",
    "    img_left = cv2.imread(\"./Stereo_3D_Reconstruction/cones1.png\")\n",
    "    img_right = cv2.imread(\"./Stereo_3D_Reconstruction/cones2.png\")\n",
    "    img_left_color = cv2.cvtColor(cv2.imread(\"./Stereo_3D_Reconstruction/cones1.png\"), cv2.COLOR_BGR2RGB) # BGR\n",
    "    point_cloud_cluster = 20\n",
    "    focal_length_1, focal_length_2 = 360.0, 360.0\n",
    "    cx, cy = 225, 188\n",
    "    NNDR_THRESHOLD = 0.9\n",
    "    \n",
    "elif IMAGE_PAIR ==\"DOLLS\":\n",
    "    img_left = cv2.imread(\"./Stereo_3D_Reconstruction/dolls1.png\")\n",
    "    img_right = cv2.imread(\"./Stereo_3D_Reconstruction/dolls2.png\")\n",
    "    img_left_color = cv2.cvtColor(cv2.imread(\"./Stereo_3D_Reconstruction/dolls1.png\"), cv2.COLOR_BGR2RGB) # BGR\n",
    "    point_cloud_cluster = 300\n",
    "    focal_length_1, focal_length_2 = 1512.0, 1512.0\n",
    "    cx, cy = 695, 555\n",
    "    NNDR_THRESHOLD = 0.85\n",
    "    \n",
    "elif IMAGE_PAIR ==\"MOEBIUS\":\n",
    "    img_left = cv2.imread(\"./Stereo_3D_Reconstruction/moebius1.png\")\n",
    "    img_right = cv2.imread(\"./Stereo_3D_Reconstruction/moebius2.png\")\n",
    "    img_left_color = cv2.cvtColor(cv2.imread(\"./Stereo_3D_Reconstruction/moebius1.png\"), cv2.COLOR_BGR2RGB) # BGR\n",
    "    point_cloud_cluster = 100\n",
    "    focal_length_1, focal_length_2 = 1512.0, 1512.0\n",
    "    cx, cy = 695, 555\n",
    "    NNDR_THRESHOLD = 0.77\n",
    "    \n",
    "elif IMAGE_PAIR ==\"ROCKS\":\n",
    "    img_left = cv2.imread(\"./Stereo_3D_Reconstruction/rocks1.png\")\n",
    "    img_right = cv2.imread(\"./Stereo_3D_Reconstruction/rocks2.png\")\n",
    "    img_left_color = cv2.cvtColor(cv2.imread(\"./Stereo_3D_Reconstruction/rocks1.png\"), cv2.COLOR_BGR2RGB) # BGR\n",
    "    point_cloud_cluster = 30\n",
    "    focal_length_1, focal_length_2 = 1512.0, 1512.0\n",
    "    cx, cy = 638, 555\n",
    "    NNDR_THRESHOLD = 0.9\n",
    "    \n",
    "elif IMAGE_PAIR ==\"CLOTH\":\n",
    "    img_left = cv2.imread(\"./Stereo_3D_Reconstruction/cloth1.png\")\n",
    "    img_right = cv2.imread(\"./Stereo_3D_Reconstruction/cloth2.png\")\n",
    "    img_left_color = cv2.cvtColor(cv2.imread(\"./Stereo_3D_Reconstruction/cloth1.png\"), cv2.COLOR_BGR2RGB) # BGR\n",
    "    point_cloud_cluster = 300\n",
    "    focal_length_1, focal_length_2 = 1050.0, 1050.0\n",
    "    cx, cy = 641, 555\n",
    "    NNDR_THRESHOLD = 0.9\n",
    "    \n",
    "########################################### Camera Intrinsic Matrix\n",
    "K = np.array([[focal_length_1, 0, cx],\n",
    "              [0, focal_length_2, cy],\n",
    "              [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f0f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Import ESPNet Model Architecture\n",
    "from espnet_model import ESPNet, ESPNet_Encoder, InputProjectionA, DilatedParllelResidualBlockB, DownSamplerB, CDilated, C, CB, BR, CBR\n",
    "model = ESPNet().to(device)\n",
    "\n",
    "### Load DEEP DETECT Model Trained for 3D Reconstruction\n",
    "model = torch.load(\"DEEP_DETECT_Best_Model_3DReconstruction.pth\", weights_only=False, map_location=torch.device('cuda'))\n",
    "\n",
    "nndr_threshold = 0.99\n",
    "my_threshold = 1.0\n",
    "model_threshold = 0.5   ### Model's Prediction Threshold (Tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccd53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: 107834 keypoints \n",
      "Image 2: 106248 keypoints\n"
     ]
    }
   ],
   "source": [
    "height_1, width_1 = img_left.shape[:2]\n",
    "height_2, width_2 = img_right.shape[:2]\n",
    "\n",
    "### Resize both to 480x480 pixels\n",
    "img1 = cv2.resize(img_left, (480, 480), interpolation=cv2.INTER_CUBIC)\n",
    "img2 = cv2.resize(img_right, (480, 480), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "img_1 = Image.fromarray(img1)\n",
    "img_2 = Image.fromarray(img2)\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "input_tensor_1 = transform(img_1).unsqueeze(0).to(device)  # [1, 3, 480, 480]\n",
    "input_tensor_2 = transform(img_2).unsqueeze(0).to(device)  # [1, 3, 480, 480]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_1, pred_2 = model(input_tensor_1), model(input_tensor_2)  # [1, 1, 480, 480]\n",
    "    pred_1, pred_2 = torch.sigmoid(pred_1), torch.sigmoid(pred_2)  # Converting Logits to Probabilities\n",
    "\n",
    "mask_1 = pred_1.cpu().squeeze().numpy()\n",
    "mask_2 = pred_2.cpu().squeeze().numpy()\n",
    "mask_1 = cv2.resize(mask_1, (width_1, height_1), interpolation=cv2.INTER_CUBIC)\n",
    "mask_2 = cv2.resize(mask_2, (width_2, height_2), interpolation=cv2.INTER_CUBIC)\n",
    "mask_1 = (mask_1 > model_threshold).astype(np.uint8)\n",
    "mask_2 = (mask_2 > model_threshold).astype(np.uint8)\n",
    "\n",
    "def mask_to_keypoints(mask, size=3):\n",
    "    ys, xs = np.where(mask == 1) ### Getting Coordinates of ONES\n",
    "    keypoints = [cv2.KeyPoint(float(x), float(y), size) for (y, x) in zip(ys, xs)]\n",
    "    return keypoints\n",
    "\n",
    "kp1_list = mask_to_keypoints(mask_1) ### Creating Keypoints from Masks\n",
    "kp2_list = mask_to_keypoints(mask_2)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.compute(img_left, kp1_list, None)  ### Computing SIFT Descriptors for Detected Keypoints\n",
    "kp2, des2 = sift.compute(img_right, kp2_list, None) ### Computing SIFT Descriptors for Detected Keypoints\n",
    "\n",
    "print(f\"Image 1: {len(kp1)} keypoints \\nImage 2: {len(kp2)} keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602ae98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of Matches: 55820\n"
     ]
    }
   ],
   "source": [
    "########################################### Feature Matching (FLANN)\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=10)\n",
    "search_params = dict(checks=100)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "### NNDR based Matching (Lowe's Ratio Test)\n",
    "good_matches = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "for m, n in matches:\n",
    "    if m.distance < NNDR_THRESHOLD * n.distance:\n",
    "        good_matches.append(m)\n",
    "        pts1.append(kp1[m.queryIdx].pt)\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "pts1 = np.array(pts1)\n",
    "pts2 = np.array(pts2)\n",
    "print(f\"Quantity of Matches: {len(good_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b84a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Matches after Outlier Rejection: 49979\n"
     ]
    }
   ],
   "source": [
    "########################################### Estimate Essential Matrix\n",
    "E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.99999, threshold=0.5, maxIters=100000)\n",
    "\n",
    "pts1 = pts1[mask.ravel() == 1]\n",
    "pts2 = pts2[mask.ravel() == 1]\n",
    "\n",
    "print(f\"Good Matches after Outlier Rejection: {pts1.shape[0]}\")\n",
    "\n",
    "### Recovering Camera Pose\n",
    "_, R, t, _ = cv2.recoverPose(E, pts1, pts2, K)\n",
    "\n",
    "### Projection matrices\n",
    "P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "P2 = K @ np.hstack((R, t))\n",
    "\n",
    "### Triangulation\n",
    "pts1_h = pts1.T\n",
    "pts2_h = pts2.T\n",
    "\n",
    "points_4d = cv2.triangulatePoints(P1, P2, pts1_h, pts2_h)\n",
    "points_3d = points_4d[:3] / points_4d[3]\n",
    "points_3d = points_3d.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072388f",
   "metadata": {},
   "source": [
    "### **Colored 3D Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d85cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for Projecting Points\n",
    "def project_points(points_3d, K):\n",
    "    points_h = np.hstack((points_3d, np.ones((points_3d.shape[0], 1))))\n",
    "    proj = (K @ points_h[:, :3].T).T\n",
    "    proj = proj[:, :2] / proj[:, 2:3]\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f8ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w, _ = img_left_color.shape\n",
    "projected_pts = project_points(points_3d, K)\n",
    "colors, valid_colors, valid_points = [], [], []\n",
    "\n",
    "for i, (u, v) in enumerate(projected_pts):\n",
    "    u, v = int(round(u)), int(round(v))\n",
    "    if 0 <= u < w and 0 <= v < h:\n",
    "        color = img_left_color[v, u] / 255.0  # normalize\n",
    "        valid_points.append(points_3d[i])\n",
    "        valid_colors.append(color)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(np.array(valid_points))\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.array(valid_colors))\n",
    "\n",
    "### Remove Outliers\n",
    "pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=point_cloud_cluster, std_ratio=0.5)\n",
    "\n",
    "### Front View\n",
    "o3d.visualization.draw_geometries([pcd], zoom=0.1, front=[0.0, 0.0, -1.0], lookat=[0.0, 0.0, 0.0], up=[0.0, -1.0, 0.0])\n",
    "\n",
    "### Create Visualizer\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=True, width=1280, height=720)\n",
    "vis.add_geometry(pcd)\n",
    "\n",
    "# Access Camera and Set Camera Parameters\n",
    "ctr = vis.get_view_control()\n",
    "ctr.set_zoom(0.1)\n",
    "ctr.set_front([0.0, 0.0, -1.0])\n",
    "ctr.set_lookat([0.0, 0.0, 0.0])\n",
    "ctr.set_up([0.0, -1.0, 0.0])\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "### Save 2D Snapshot\n",
    "vis.capture_screen_image(\"point_cloud_DEEP_DETECT.png\")\n",
    "vis.destroy_window() # Cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofa_adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
